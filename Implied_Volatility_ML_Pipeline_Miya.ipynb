{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n#Implied Volatility Machine Learning Pipeline Application\nThis notebook is an end-to-end exercise of performing Extract-Transform-Load and Exploratory Data Analysis on a real-world dataset, and then applying several different machine learning algorithms to solve a supervised regression problem on the dataset.\n\n** This notebook covers: **\n* *Part 1: Load Your Data*\n* *Part 2: Explore Your Data*\n* *Part 3: Visualize Your Data*\n* *Part 4: Data Preparation*\n* *Part 5: Data Modeling*\n* *Part 6: Tuning and Evaluation*"],"metadata":{}},{"cell_type":"markdown","source":["## Part 1: Extract-Transform-Load (ETL) Data\n\nOur data is available on Amazon s3 at the following path:\n\n```\n/FileStore/tables/gtd0kb9j1481295433792/cisc5352_project_1_option_data-f2b26.csv```\n\n**To Do:** Let's start by printing a sample of the data.\n\nWe'll use the built-in Databricks functions for exploring the Databricks filesystem (DBFS)"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/FileStore/tables/gtd0kb9j1481295433792/cisc5352_project_1_option_data-f2b26.csv\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Next, use the `dbutils.fs.head` command to look at the first 65,536 bytes of the first file in the directory."],"metadata":{}},{"cell_type":"markdown","source":["`dbutils.fs` has its own help facility, which we can use to see the various available functions."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Now, let's use PySpark instead to print the first 5 lines of the data."],"metadata":{}},{"cell_type":"code","source":["# TODO: Load the data and print the first five lines.\nrawRdd = sc.textFile(\"/FileStore/tables/gtd0kb9j1481295433792/cisc5352_project_1_option_data-f2b26.csv\") \nrawRdd.take(5)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["From our initial exploration of a sample of the data, we can make several observations for the ETL process:\n  - The data is a set of .tsv (Tab Seperated Values) files (i.e., each row of the data is separated using tabs)\n  - There is a header row, which is the name of the columns\n  - It looks like the type of the data in each column is consistent (i.e., each column is of type double or int)\n\nOur schema appears below:\n- Stock Price\n- Strike Price\n- Time to Maturity\n- Interest Rate\n- Option Price\n- Volatility\n- Implied Volatility\n\n(**Note**: In Spark 2.0, the CSV package is built into the DataFrame API.)"],"metadata":{}},{"cell_type":"code","source":["stockDF=sqlContext.read.format('com.databricks.spark.csv').options(delimiter=',',header='true',inferschema='true').load(\"/FileStore/tables/gtd0kb9j1481295433792/cisc5352_project_1_option_data-f2b26.csv\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print stockDF.dtypes"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["We can examine the data using the display() method."],"metadata":{}},{"cell_type":"code","source":["display(stockDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Part 2: Explore Data"],"metadata":{}},{"cell_type":"markdown","source":["First, let's register our DataFrame as an SQL table named `stock_table`.  Because you may run this lab multiple times, we'll take the precaution of removing any existing tables first.\n\nWe can delete any existing `stock_table` SQL table using the SQL command: `DROP TABLE IF EXISTS stock_table` (we also need to to delete any Hive data associated with the table, which we can do with a Databricks file system operation).\n\nOnce any prior table is removed, we can register our DataFrame as a SQL table using [sqlContext.registerDataFrameAsTable()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext.registerDataFrameAsTable)."],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql(\"DROP TABLE IF EXISTS stock_table\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/tock_table\", True)\nsqlContext.registerDataFrameAsTable(stockDF, \"stock_table\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Now that our DataFrame exists as a SQL table, we can explore it using SQL commands.\n\nTo execute SQL in a cell, we use the `%sql` operator. The following cell is an example of using SQL to query the rows of the SQL table."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- We can use %sql to query the rows\nSELECT * FROM stock_table"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Use the SQL `desc` command to describe the schema, by executing the following cell."],"metadata":{}},{"cell_type":"code","source":["%sql\ndesc stock_table"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Let's perform some basic statistical analyses of all the columns."],"metadata":{}},{"cell_type":"code","source":["df = sqlContext.table(\"stock_table\")\ndisplay(df.describe())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import *\n\ndf = df.select(\"Stock_Price\", \"Strike_Price\", \"Option_Price\",df['Option_Type'].cast(DoubleType()),'Volatility',col('Implied Volatility').alias('IV'),df[\"Time_to_Maturity\"].cast(DoubleType()))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df.dtypes"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["##Part 4: Data Preparation\n\nThe next step is to prepare the data for machine learning. Since all of this data is numeric and consistent this is a simple and straightforward task.\n\nThe goal is to use machine learning to determine a function that yields the output power as a function of a set of predictor features. The first step in building our ML pipeline is to convert the predictor features from DataFrame columns to Feature Vectors using the [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) method.\n\nThe VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler takes a list of input column names (each is a string) and the name of the output column (as a string)."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code\nfrom pyspark.ml.feature import VectorAssembler\n\ndatasetDF = df\n\nvectorizer = VectorAssembler()\nvectorizer.setInputCols([\"Stock_Price\", \"Strike_Price\", \"Time_to_Maturity\",\"Option_Price\",'Option_Type','Volatility']) \nvectorizer.setOutputCol(\"features\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["##Part 5: Data Modeling\n\nOur first model will be based on simple linear regression since we saw some linear patterns in our data based on the scatter plots during the exploration stage.\n\nWe need a way of evaluating how well our linear regression model predicts power output as a function of input parameters. We can do this by splitting up our initial data set into a _Training Set_ used to train our model and a _Test Set_ used to evaluate the model's performance in giving predictions. We can use a DataFrame's [randomSplit()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) method to split our dataset. The method takes a list of weights and an optional random seed. The seed is used to initialize the random number generator used by the splitting function."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n# We'll hold out 20% of our data for testing and leave 80% for training\nseed = 1800009193L\n(split20DF, split80DF) = datasetDF.randomSplit([0.2,0.8],seed)\n\n# Let's cache these datasets for performance\ntestSetDF = split20DF.cache()\ntrainingSetDF = split80DF.cache()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Next we'll create a Linear Regression Model and use the built in help to identify how to train it. See API details for [Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) in the ML guide."],"metadata":{}},{"cell_type":"code","source":["# ***** LINEAR REGRESSION MODEL ****\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.regression import LinearRegressionModel\nfrom pyspark.ml import Pipeline\n\n# Let's initialize our linear regression learner\nlr = LinearRegression()\n\n# We use explain params to dump the parameters we can use\nprint(lr.explainParams())"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["The cell below is based on the [Spark ML Pipeline API for Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression)."],"metadata":{}},{"cell_type":"code","source":["# Now we set the parameters for the method\nlr.setLabelCol('IV')\\\n  .setMaxIter(100)\\\n  .setRegParam(0.1)\n\n\n# We will use the new spark.ml pipeline API.\nlrPipeline = Pipeline()\n\nlrPipeline.setStages([vectorizer, lr])\n\n# Let's first train on the entire dataset to see what we get\nlrModel = lrPipeline.fit(trainingSetDF)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# The intercept is as follows:\nintercept = lrModel.stages[1].intercept\n\n# The coefficents (i.e., weights) are as follows:\nweights = lrModel.stages[1].coefficients\n\n# Create a list of the column names (without PE)\nfeaturesNoLabel = [col for col in datasetDF.columns if col != \"IV\"]\n\n# Merge the weights and labels\ncoefficents = zip(weights, featuresNoLabel)\n\n# Now let's sort the coefficients from greatest absolute weight most to the least absolute weight\ncoefficents.sort(key=lambda tup: tup[0], reverse=True)\n\nequation = \"y = {intercept}\".format(intercept=intercept)\nvariables = []\nfor x in coefficents:\n    weight = x[0]\n    name = x[1]\n    symbol = \"+\" if (x[0] > 0) else \"-\"\n    equation += (\" {} ({} * {})\".format(symbol, weight, name))\n\n# Finally here is our equation\nprint(\"Linear Regression Equation: \" + equation)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Apply our LR model to the test data and predict power output\npredictionsAndLabelsDF = lrModel.transform(testSetDF).select(\"Stock_Price\", \"Strike_Price\", \"Time_to_Maturity\",\"Option_Price\",'Option_Type','Volatility','IV',\"prediction\")\n\ndisplay(predictionsAndLabelsDF)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Now let's compute an evaluation metric for our test dataset\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"IV\", metricName=\"rmse\")\n\n# Run the evaluator on the DataFrame\nrmse = regEval.evaluate(predictionsAndLabelsDF)\n\nprint(\"Root Mean Squared Error: %.2f\" % rmse)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Another useful statistical evaluation metric is the coefficient of determination, denoted \\\\(R^2\\\\) or \\\\(r^2\\\\) and pronounced \"R squared\". It is a number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable and it provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. The coefficient of determination ranges from 0 to 1 (closer to 1), and the higher the value, the better our model."],"metadata":{}},{"cell_type":"code","source":["# Now let's compute another evaluation metric for our test dataset\nr2 = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"r2: {0:.2f}\".format(r2))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Generally, assuming a Gaussian distribution of errors, a good model will have 68% of predictions within 1 RMSE and 95% within 2 RMSE of the actual value (see http://statweb.stanford.edu/~susan/courses/s60/split/node60.html).\n\nLet's examine the predictions and see if a RMSE of 0.15 meets this criteria."],"metadata":{}},{"cell_type":"code","source":["# First we remove the table if it already exists\nsqlContext.sql(\"DROP TABLE IF EXISTS stock_table_RMSE_Evaluation\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/stock_table_RMSE_Evaluation\", True)\n\n# Next we calculate the residual error and divide it by the RMSE\npredictionsAndLabelsDF.selectExpr(\"IV\", \"prediction\", \"IV - prediction Residual_Error\",\"(IV - prediction) /{} Within_RSME\".format(rmse)).registerTempTable(\"stock_table_RMSE_Evaluation\")"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["We can use SQL to explore the `Power_Plant_RMSE_Evaluation` table. First let's look at at the table using a SQL SELECT statement."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * from stock_table_RMSE_Evaluation"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Now we can display the RMSE as a Histogram.\n\nNotice that the histogram clearly shows that the RMSE is centered around 0."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Now we can display the RMSE as a Histogram\nSELECT Within_RSME  from stock_table_RMSE_Evaluation"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["Using a complex SQL SELECT statement, we can count the number of predictions within + or - 1.0 and + or - 2.0 and then display the results as a pie chart."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1\n            when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3\n       end RSME_Multiple, COUNT(*) AS count\nFROM stock_table_RMSE_Evaluation\nGROUP BY case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1  when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3 end"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["From the pie chart, we can see that 68% of our test data predictions are within 1 RMSE of the actual values, and 97% (68% + 29%) of our test data predictions are within 2 RMSE. So the model is pretty decent. Let's see if we can tune the model to improve it further."],"metadata":{}},{"cell_type":"markdown","source":["##Part 6: Tuning and Evaluation\n\nWe perform the following steps:\n  - Create a [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) using the Pipeline and [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) that we created earlier, and set the number of folds to 3\n  - Create a list of 10 regularization parameters\n  - Use [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to build a parameter grid with the regularization parameters and add the grid to the [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation)\n  - Run the [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) to find the parameters that yield the best model (i.e., lowest RMSE) and return the best model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# We can reuse the RegressionEvaluator, regEval, to judge the model based on the best Root Mean Squared Error\n# Let's create our CrossValidator with 3 fold cross validation\ncrossval = CrossValidator(estimator=lrPipeline, evaluator=regEval, numFolds=3)\n\n# Let's tune over our regularization parameter from 0.01 to 0.10\nregParam = [x / 100.0 for x in range(1, 11)]\n\n# We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, regParam)\n             .build())\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\ncvModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["Now that we have tuned our Linear Regression model, let's see what the new RMSE and \\\\(r^2\\\\) values are versus our intial model.\n\n### Exercise 7(b)\n\nComplete and run the next cell."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n# Now let's use cvModel to compute an evaluation metric for our test dataset: testSetDF\npredictionsAndLabelsDF = cvModel.transform(testSetDF).select(\"Stock_Price\", \"Strike_Price\", \"Time_to_Maturity\",\"Option_Price\",'Option_Type','Volatility','IV',\"prediction\")\n\n# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\nrmseNew = regEval.evaluate(predictionsAndLabelsDF)\n\n# Now let's compute the r2 evaluation metric for our test dataset\nr2New = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"Original Root Mean Squared Error: {0:2.2f}\".format(rmse))\nprint(\"New Root Mean Squared Error: {0:2.2f}\".format(rmseNew))\nprint(\"Old r2: {0:2.2f}\".format(r2))\nprint(\"New r2: {0:2.2f}\".format(r2New))"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["[Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning) uses a [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree) as a predictive model which maps observations about an item to conclusions about the item's target value. It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n\nSpark ML Pipeline provides [DecisionTreeRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) as an implementation of [Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning).\n\nThe cell below is based on the [Spark ML Pipeline API for Decision Tree Regressor](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor)."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\nfrom pyspark.ml.regression import DecisionTreeRegressor\n\n# Create a DecisionTreeRegressor\ndt = DecisionTreeRegressor()\n\ndt.setLabelCol(\"IV\")\\\n  .setPredictionCol(\"prediction\")\\\n  .setFeaturesCol(\"features\")\\\n  .setMaxBins(100)\n\n# Create a Pipeline\ndtPipeline = Pipeline()\n\n\n# Set the stages of the Pipeline\ndtPipeline.setStages([vectorizer, dt])"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Instead guessing what parameters to use, we will use [Model Selection](https://spark.apache.org/docs/1.6.2/ml-tuning.html#model-selection-aka-hyperparameter-tuning) or [Hyperparameter Tuning](https://spark.apache.org/docs/1.6.2/ml-tuning.html#model-selection-aka-hyperparameter-tuning) to create the best model.\n\nWe can reuse the exiting [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) by replacing the Estimator with our new `dtPipeline` (the number of folds remains 3)."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n# Let's just reuse our CrossValidator with the new dtPipeline,  RegressionEvaluator regEval, and 3 fold cross validation\ncrossval.setEstimator(dtPipeline)\n\n# Let's tune over our dt.maxDepth parameter on the values 2 and 3, create a paramter grid using the ParamGridBuilder\nparamGrid = ParamGridBuilder().addGrid(dt.maxDepth, [2, 3]).build()\n# Add the grid to the CrossValidator\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\ndtModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["Now let's see how our tuned DecisionTreeRegressor model's RMSE and \\\\(r^2\\\\) values compare to our tuned LinearRegression model."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n\n# Now let's use dtModel to compute an evaluation metric for our test dataset: testSetDF\npredictionsAndLabelsDF = dtModel.transform(testSetDF).select(\"Stock_Price\", \"Strike_Price\", \"Time_to_Maturity\",\"Option_Price\",'Option_Type','Volatility','IV', \"prediction\")\n\n# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\nrmseDT =  regEval.evaluate(predictionsAndLabelsDF)\n\n# Now let's compute the r2 evaluation metric for our test dataset\nr2DT = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"LR Root Mean Squared Error: {0:.2f}\".format(rmseNew))\nprint(\"DT Root Mean Squared Error: {0:.2f}\".format(rmseDT))\nprint(\"LR r2: {0:.2f}\".format(r2New))\nprint(\"DT r2: {0:.2f}\".format(r2DT))"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["The line below will pull the Decision Tree model from the Pipeline as display it as an if-then-else string. Again, we have to \"reach through\" to the JVM API to make this one work.\n\n**ToDo**: Run the next cell"],"metadata":{}},{"cell_type":"code","source":["print dtModel.stages[-1]._java_obj.toDebugString()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["[Random forests](https://en.wikipedia.org/wiki/Random_forest) or random decision tree forests are an ensemble learning method for regression that operate by constructing a multitude of decision trees at training time and outputting the class that is the mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nSpark ML Pipeline provides [RandomForestRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor) as an implementation of [Random forests](https://en.wikipedia.org/wiki/Random_forest)."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n\nfrom pyspark.ml.regression import RandomForestRegressor\n\n# Create a RandomForestRegressor\nrf = RandomForestRegressor()\n\nrf.setLabelCol(\"IV\")\\\n  .setPredictionCol(\"prediction\")\\\n  .setFeaturesCol(\"features\")\\\n  .setSeed(100088121L)\\\n  .setMaxDepth(8)\\\n  .setNumTrees(30)\n\n# Create a Pipeline\nrfPipeline = Pipeline()\n\n# Set the stages of the Pipeline\nrfPipeline.setStages([vectorizer, rf])"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["As with Decision Trees, instead guessing what parameters to use, we will use [Model Selection](https://spark.apache.org/docs/1.6.2/ml-tuning.html#model-selection-aka-hyperparameter-tuning) or [Hyperparameter Tuning](https://spark.apache.org/docs/1.6.2/ml-tuning.html#model-selection-aka-hyperparameter-tuning) to create the best model.\n\nWe can reuse the exiting [CrossValidator](https://spark.apache.org/docs/1.6.2/ml-tuning.html#cross-validation) by replacing the Estimator with our new `rfPipeline` (the number of folds remains 3)."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n# Let's just reuse our CrossValidator with the new rfPipeline,  RegressionEvaluator regEval, and 3 fold cross validation\ncrossval.setEstimator(rfPipeline)\n\n# Let's tune over our rf.maxBins parameter on the values 50 and 100, create a paramter grid using the ParamGridBuilder\nparamGrid = ParamGridBuilder().addGrid(rf.maxBins, [50, 100]).build()\n\n# Add the grid to the CrossValidator\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\nrfModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["Now let's see how our tuned RandomForestRegressor model's RMSE and \\\\(r^2\\\\) values compare to our tuned LinearRegression and tuned DecisionTreeRegressor models.\n\nComplete and run the next cell."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n\n# Now let's use rfModel to compute an evaluation metric for our test dataset: testSetDF\npredictionsAndLabelsDF = rfModel.transform(testSetDF).select(\"Stock_Price\", \"Strike_Price\", \"Time_to_Maturity\",\"Option_Price\",'Option_Type','Volatility','IV', \"prediction\")\n\n# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\nrmseRF = regEval.evaluate(predictionsAndLabelsDF)\n\n# Now let's compute the r2 evaluation metric for our test dataset\nr2RF = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"LR Root Mean Squared Error: {0:.2f}\".format(rmseNew))\nprint(\"DT Root Mean Squared Error: {0:.2f}\".format(rmseDT))\nprint(\"RF Root Mean Squared Error: {0:.2f}\".format(rmseRF))\nprint(\"LR r2: {0:.2f}\".format(r2New))\nprint(\"DT r2: {0:.2f}\".format(r2DT))\nprint(\"RF r2: {0:.2f}\".format(r2RF))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["print rfModel.stages[-1]._java_obj.toDebugString()"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["### Conclusion\n\nBest model is in fact our Random Forest tree model which uses an ensemble of 30 Trees with a depth of 8 to construct a better model than the single decision tree."],"metadata":{}}],"metadata":{"name":"Implied_Volatility_ML_Pipeline_Miya","notebookId":2017947690024530},"nbformat":4,"nbformat_minor":0}
